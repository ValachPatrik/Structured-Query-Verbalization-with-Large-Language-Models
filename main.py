import os
import re
import requests
import json
import pandas as pd
from typing import *
from wikidata.client import Client
from datasets import load_dataset

def find_translatable_parts(sparql_query: str) -> List[str]:
    """
    This function takes a SPARQL query as input and returns a list of all the entities and properties that are translatable.

    Parameters:
    sparql_query (str): The SPARQL query to be processed

    Returns:
    List[str]: A list of all the entities and properties that are translatable in the SPARQL query
    """
    entity_pattern = re.compile(r'wd:Q\d+')
    property_pattern = re.compile(r'wdt:P\d+')

    entity_matches = entity_pattern.findall(sparql_query)
    property_matches = property_pattern.findall(sparql_query)

    return entity_matches + property_matches

def map_wikidata_to_natural_language(sparql_query:str) -> str:
    """
    This function takes a SPARQL query as input and returns a modified SPARQL query where all the entities and properties that are translatable are replaced with their corresponding natural language labels.

    Parameters:
    sparql_query (str): The SPARQL query to be processed.

    Returns:
    str: A modified SPARQL query where all the entities and properties that are translatable are replaced with their corresponding natural language labels.

    Raises:
    Exception: If an error occurs while retrieving the label of an entity or property and leaves the query as is.
    """
    client = Client()
    
    translatable_parts = find_translatable_parts(sparql_query)
    
    for i in translatable_parts:
        try:
            entity = client.get(i.split(":")[1], load=True)
            sparql_query = sparql_query.replace(i, str(entity.label))
        except Exception as e:
            print(e, end=" ")
            print(i)
    return sparql_query
    

def load_lc() -> pd.DataFrame:
    """
    This function loads the LC quad dataset. If the dataset is already locally available, it loads it from the local storage.
    Otherwise, it fetches the data from the network.
    
    Deletes rows with failed mappings!!!

    Parameters:
    None

    Returns:
    pd.DataFrame: A pandas DataFrame containing the LC quad dataset.
    """
    if os.path.exists("lc_quad_translated.csv"):
        # Load locally if data is already locally ready
        combined_df = pd.read_csv("lc_quad_translated.csv")
        print("Loaded from local")
    else:
        # Load data from net
        lc_quad_dataset = load_dataset("lc_quad")

        # combine both datasets test and train, we dont need this kind of differentiation
        train_df = lc_quad_dataset["train"].to_pandas()
        test_df = lc_quad_dataset["test"].to_pandas()
        combined_df = pd.concat([train_df, test_df], ignore_index=True)
        
        LOAD_LIMIT = 1000
        combined_df = combined_df.head(LOAD_LIMIT)

        # Apply KG to map from ambiguous descriptions to NL
        combined_df["wikidata_translated"] = combined_df["sparql_wikidata"].map(map_wikidata_to_natural_language)
        
        # Delete failed rows
        combined_df = combined_df[combined_df["wikidata_translated"] != combined_df["sparql_wikidata"]]
        
        # Save DF for future use to save resources
        combined_df.to_csv("lc_quad_translated.csv", index=False)
        
        print("Loaded from net, load limit is {LOAD_LIMIT}")
    print(f"Loaded {len(combined_df.index)} rows")
    return combined_df






def generate_response_llama(sparql_query: str, model: str, api_endpoint: str) -> str:
    """
    This function sends a request to an LLM API to translate a given SPARQL query into natural language.

    Parameters:
    sparql_query (str): The SPARQL query to be translated.
    model (str): The name of the LLM model to be used for translation.

    Returns:
    str: The translated response generated by the LLM model.
    """

    payload = {
        "model": model,
        "prompt": "Translate the sparql query into natural language; formulate the response as a question and respond in one sentence only with the translation itself: " + sparql_query
    }
    response = requests.post(api_endpoint, json=payload, stream=True)
    response_text = ""
    for line in response.text.split("\n"):
        try:
            response_text += line.split('response":"')[1].split('","done":')[0]
        except:
            pass
    #print(response_text)
    return response_text
    
def use_llm(df: pd.DataFrame, model: str, api_endpoint: str) -> pd.DataFrame:
    """
    Takes a dataframe with the wikidata_translated column and applies the LLM model to it.

    Parameters:
    df (pd.DataFrame): The dataframe containing the wikidata_translated column.
    model (str): The name of the LLM model to be used.

    Returns:
    pd.DataFrame: The input dataframe with an additional column containing the translated responses generated by the LLM model.
    """
    if os.path.exists(f"lc_quad_translated_{model}.csv"):
        # Load locally if data is already locally ready
        df = pd.read_csv(f"lc_quad_translated_{model}.csv")
        print("Loaded from local model responses")
    else:
        df[f'{model}_response'] = df['wikidata_translated'].apply(lambda x: generate_response_llama(x, model, api_endpoint))
    
        df.to_csv(f"lc_quad_translated_{model}.csv", index=False)
        print(f"Saved {len(df.index)} rows with model responses")
    
    print(f"Loaded {len(df.index)} rows")
    return df





def eval_manual(df: pd.DataFrame, model: str, size_manual_eval: int) -> pd.DataFrame:
    if os.path.exists(f"lc_quad_translated_{model}_evaluated.csv"):
        # Load locally if data is already locally ready
        print(f"Evaluated file already exists. Do you want to overwrite/add manual eval? (y/n)")
        answer = input()
        if answer == "y":
            df = eval_manual_logic(df, model, size_manual_eval)
        else:
            df = pd.read_csv(f"lc_quad_translated_{model}_evaluated.csv")
    else:
        df = eval_manual_logic(df, model, size_manual_eval)
    
    df.to_csv(f"lc_quad_translated_{model}_evaluated.csv", index=False)
    print("Saved manual eval")
    return df

def eval_manual_logic(df: pd.DataFrame, model: str, size_manual_eval: int) -> pd.DataFrame:
    print("Creating manual evaluation")
    print(size_manual_eval)
    for i in range(size_manual_eval):
        print(i)
        print(f"Dataset: {df.at[i, 'question']}")
        print(f"Model: {df.at[i, f'{model}_response']}")
        print("Are these two the same? (y/n) ", end="")
        response = input()
        print()
        if response == "y":
            df.at[i, 'eval_manual'] = 1
        else:
            df.at[i, 'eval_manual'] = 0
    print("Manual eval created")
    return df



def eval_llm(df: pd.DataFrame, model: str, api_endpoint: str) -> pd.DataFrame:
    df = df.head(100)
    if os.path.exists(f"lc_quad_translated_{model}_evaluated.csv"):
        # Load locally if data is already locally ready
        print(f"Evaluated file already exists. Do you want to overwrite/add llm eval? (y/n)")
        answer = input()
        if answer == "y":
            df['eval_llm'] = df.apply(lambda x: eval_llm_logic(x['paraphrased_question'], x[f'{model}_response'], model, api_endpoint), axis=1)
        else:
            df = pd.read_csv(f"lc_quad_translated_{model}_evaluated.csv")
    else:
        df['eval_llm'] = df.apply(lambda x: eval_llm_logic(x["paraphrased_question"], x[f"{model}_response"], model, api_endpoint), axis=1)
    
    df.to_csv(f"lc_quad_translated_{model}_evaluated.csv", index=False)
    print("Saved llm eval")
    return df
    
def eval_llm_logic(dataset: str, model_response: str, model: str, api_endpoint: str) -> int:
    payload = {
        "model": model,
        "prompt": "Respond with a single word. Are the following sentences semantically the same?\n" + str(dataset) + "\n" + str(model_response)
    }
    response = requests.post(api_endpoint, json=payload, stream=True)
    response_text = ""
    for line in response.text.split("\n"):
        try:
            response_text += line.split('response":"')[1].split('","done":')[0]
        except:
            pass

    print(response_text)
    if response_text[:3].lower() == "yes":
        return 1
    elif response_text[:2].lower() == "no":
        return 0
    else:
        print("Something went wrong")
        print(response_text)
        return 0



def eval_mlp_bert(df: pd.DataFrame, model: str) -> pd.DataFrame:
    if os.path.exists(f"lc_quad_translated_{model}_evaluated.csv"):
        # Load locally if data is already locally ready
        print(f"Evaluated file already exists. Do you want to overwrite/add bert eval? (y/n)")
        answer = input()
        if answer == "y":
            df['eval_bert'] = df.apply(lambda x: eval_bert_logic(x["paraphrased_question"], x[f"{model}_response"]), axis=1)
        else:
            df = pd.read_csv(f"lc_quad_translated_{model}_evaluated.csv")
    else:
        df['eval_bert'] = df.apply(lambda x: eval_bert_logic(x["paraphrased_question"], x[f"{model}_response"]), axis=1)
    
    df.to_csv(f"lc_quad_translated_{model}_evaluated.csv", index=False)
    print("Saved bert eval")
    return df

def eval_bert_logic(dataset: str, model_response: str):
    pass

def eval_mlp_bleu(df: pd.DataFrame, model: str) -> pd.DataFrame:
    if os.path.exists(f"lc_quad_translated_{model}_evaluated.csv"):
        # Load locally if data is already locally ready
        print(f"Evaluated file already exists. Do you want to overwrite/add bleu eval? (y/n)")
        answer = input()
        if answer == "y":
            df['eval_bert'] = df.apply(lambda x: eval_bleu_logic(x["paraphrased_question"], x[f"{model}_response"]))
        else:
            df = pd.read_csv(f"lc_quad_translated_{model}_evaluated.csv")
    else:
        df['eval_bert'] = df.apply(lambda x: eval_bleu_logic(x["paraphrased_question"], x[f"{model}_response"]))
    
    df.to_csv(f"lc_quad_translated_{model}_evaluated.csv", index=False)
    print("Saved bleu eval")
    return df

def eval_bleu_logic(dataset: str, model_response: str):
    pass

def accuracy(correct, total):
    return correct/total

def precision(true_positives, false_positives):
    return true_positives / (true_positives + false_positives)

def recall(true_positives, false_negatives):
    return true_positives / (true_positives + false_negatives)

def f1(precision, recall):
    return 2*precision*recall / (precision + recall)






# Constants
model = "llama3"
api_endpoint = "http://localhost:11434/api/generate"
size_manual_eval = 100

# Load data
df = load_lc()
# Run LLM
df = use_llm(df, model, api_endpoint)
# Evaluate
    # 1 - manual 0/1
df = eval_manual(df, model, size_manual_eval)
    # 2 - LLM
df = eval_llm(df, model, api_endpoint)
    # 3 - MLP
        # BERT
df = eval_mlp_bert(df, model)
        # BLEU
df = eval_mlp_bleu(df, model)


eval_manual_llm_same = df.head(size_manual_eval)[(df['eval_manual'] == df['eval_llm'])].shape[0]
eval_manual_llm_true_positives = df.head(size_manual_eval)[(df['eval_manual'] == 1) & (df['eval_llm'] == 1)].shape[0]
eval_manual_llm_false_positives = df.head(size_manual_eval)[(df['eval_manual'] == 0) & (df['eval_llm'] == 1)].shape[0]
eval_manual_llm_false_negatives = df.head(size_manual_eval)[(df['eval_manual'] == 1) & (df['eval_llm'] == 0)].shape[0]
eval_manual_llm_true_negatives = df.head(size_manual_eval)[(df['eval_manual'] == 0) & (df['eval_llm'] == 0)].shape[0]
eval_llm_total_1 = df[df['eval_llm'] == 1].shape[0]

llm_accuracy = accuracy(eval_manual_llm_same, size_manual_eval)
llm_precision = precision(eval_manual_llm_true_positives, eval_manual_llm_false_positives)
llm_recall = recall(eval_manual_llm_true_positives, eval_manual_llm_false_negatives)
llm_f1 = f1(llm_precision, llm_recall)

print("LLM")
print(f"Accuracy: {llm_accuracy}")
print(f"Precision: {llm_precision}")
print(f"Recall: {llm_recall}")
print(f"F1 Score: {llm_f1}")
print(f"Totally evaluated as correct: {eval_llm_total_1} out of {df.shape[0]}")




